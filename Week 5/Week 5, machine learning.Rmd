---
title: "Week 5, machine learning"
author: "Ane Kleiven"
date: "2024-10-03"
output: html_document
---

<H3> Answers to questions <H3/> 

**1. A biologist has just read an article about machine learning and decided to try it on his RNA-Seq data. Since he has many more genes than samples, he decided to do feature selection by choosing the most differentially expressed genes. He did feature selection first on the entire dataset and then validated his model using cross validation. To his dismay the analysis was heavily critiqued by the reviewers. They said he had to use an independent test set. The biologist doesn't understand the criticism and comes to you. Explain to him what he did wrong and why. Write at most 250 words.**

The biologist starts the whole process by doing feature selection on the whole data set, and identifying the DE genes. He now has a data set with fewer features. In the next step, the biologist divides the data set into one test set and one training set for doing cross validation. 
Cross validation is a method where you train and validate the model multiple times, with different combinations of test data and training data. 
The biologist should have divided the data into a training set and test set before he started feature selection. In this case, the model has already seen the test data (in the feature selection) when doing cross validation. When doing machine learning, you train your model first and then test it on unseen data. You do not look at your test data upon testing, the test data must be unseen. 
In the approach the biologist uses, it’s going to look like the model works (because the model has already seen the data). We can say that the model is cheating, the cross-validation process is biased. 


**2. Let's say you want to "read" the genome sequence using machine learning. You figure you can use GWAS populations as training data with SNPs as features and traits as labels. Will this approach fly? Discuss using concepts such as omnigenics, curse of dimensionality, model complexity and overfitting. Write at most 250 words.**


In the approach described above, you look at all the SNPs at the same time. Every SNP is important, so by doing this you are taking Omnigenics seriously. The principle of Omnigenics is that there are a lot of genes responsible for the heritability of traits. This means that you must look at many SNPs to describe heritability of traits. This makes it difficult to reduce the features in the model. 
One of the biggest issues with the Omnigenics approach, is “the curse of dimensionality”. Due to the millions of SNPs, you would also need millions of observations (samples) to describe the traits. The biggest research projects performed included a couple of hundred thousands samples, but have can we collect millions? There aren’t even that many persons on the planet. 
The model described would be extremely complex. Since many traits depend on many SNPs, there would be millions of interactions between the SNPs – making the model even more complex. 
The last issue with the model is overfitting, caused by too many features in relation to traits. A model that is too complex can also lead to overfitting. Overfitting means that the model becomes kind of a look up table for the training data, which doesn’t work for generalized data. Instead of predicting, the model memorizes. 



<h3> DATA <H3/> 

```{r, warning=FALSE, message=FALSE}

#Load the count data 

load(file = "TCGA.RData")

dim(counts)

table(classes)

```
<h4> Questions <h4/> 

**How many genes and samples are in the dataset?**

There are 16739 genes and 2340 samples in the dataset 



**How many samples are there for each class?** 

The number of samples within the 13 classes are shown in the table above 



```{r}
# removing lowly expressed genes in Colon, Rectum and Stomach. Update the tables 

library(dplyr)

idx <- classes %in% c("Colon", "Rectum", "Stomach")

classes <- classes[idx] %>% factor()
counts <- counts[, idx]

# Genes must have 1024 mapped reads in at least 10 samples
counts <- counts[rowSums(counts > 2^10) > 10,]
dim(counts)

```

<h4> Compute VST expression values <h4/> 

```{r}
# compute VST expression values using DESeq2 

vst <- DESeq2::varianceStabilizingTransformation(counts)
```

```{r, warning=FALSE,message=FALSE}

# select genes with different expression across individuals 

library(ggplot2)

library(matrixStats)

standard.deviations = rowSds(vst)

tibble(StandardDeviation = standard.deviations) %>% ggplot(aes(x = StandardDeviation)) + geom_density(fill = "lightpink", alpha = 0.6) + scale_x_continuous(limits = c(0.5, 3.5))
```

```{r}
vst <- vst[standard.deviations > 1.5,]
dim(vst)
```

<H4> Create a PSA plot and color the samples according to classes <H4/> 


```{r}
cancer.pca <- cancer.pca <- prcomp(t(vst), scale = TRUE)   #transpose the data so the samples are in rows

```

```{r}
cancer.pca$x %>% ggplot(aes(PC1, PC2, col = classes)) + geom_point() + labs(title = "PCA of Cancer Data", x = "PC1", y = "PC2") + theme(plot.title = element_text(hjust = 0.5)) + scale_color_manual(values = c("Colon" = "lightslateblue", 
"Rectum" = "olivedrab3", "Stomach" = "tomato"))
```
**Based on the PCA plot, do you think it will be easy or hard to classify these samples using machine learning?** 

The stomach class is quite good clustered and separated from the other two classes. This makes it possible using machine learning to classify these samples. The samples in the two other classes are more mixed together, making it harder to use machine learning on these two classes. 


```{r, include=FALSE}

1:5
seq(1, 5)
seq(1, 5, by = 0.5)

```


```{r, include = FALSE, eval = FALSE}
# animation on cross validation 

install.packages("animation")
library(animation)
animation::cv.ani()
```


<h3> k-Nearest Neighbors (k-NN) <h3/> 

```{r, include=FALSE, eval=FALSE}
# animation on k-nearest neighbor

animation::knn.ani()

```

```{r, include=FALSE}
library(caret)
set.seed(3)

idxTrain <- createDataPartition(y = iris$Species, p = 0.5, list = FALSE)
iris.train <- iris[idxTrain,]
iris.test <- iris[-idxTrain,]

```

```{r, include=FALSE}

iris.knn.model <- train(form = Species ~ ., 
                        data = iris.train, 
                        method = "knn",
                        trControl = trainControl(method = "cv"), 
                        tuneGrid = data.frame(k = 2:15))

iris.knn.model

```

```{r}
plot(iris.knn.model)
```
**Briefly explain the output (text and plot) in the context of the input parameters to train()** 

The plot above show the results from the trained iris-model using k-nearest neighbor and cross validation. The results show that the k-value of 6,7 and 8 has the highest accuracy, and are the K-values that should be used in this model. Values of k <6 and >8 show weaker results. 

```{r, include = FALSE}
# predict the species of the plant in the test set 

iris.knn.pred <- predict(iris.knn.model, iris.test)
iris.knn.pred
```

```{r, include=FALSE}
# confusion table to see how well the model did. compare with the correct species. 

confusion <- table(iris.test$Species, iris.knn.pred, dnn = c("Real", "Predicted"))
confusion

```

```{r, include=FALSE}

# find the accuracy 

accuracy <- sum(diag(confusion)) / sum(confusion)
accuracy

```

```{r, include=FALSE}
# find the error rate 

error <- 1 - accuracy
error

```


<H4> k-NN on the cancer RNA-Seq data <H4/> 

```{r}
# transpose the data 
vst_t <- vst %>% t() %>% as.data.frame() %>% mutate(Class = classes)
```

```{r}
library(caret)
set.seed(3)

cancerTrain <- createDataPartition(y = vst_t$Class, p = 0.5, list = FALSE)
cancer.train <- vst_t[cancerTrain,]
cancer.test <- vst_t[-cancerTrain,]

```

```{r}
cancer.knn.model <- train(form = Class ~ ., 
                        data = cancer.train, 
                        method = "knn",
                        trControl = trainControl(method = "cv"), 
                        tuneGrid = data.frame(k = 2:15))

cancer.knn.model
```
```{r}
plot(cancer.knn.model)
```

```{r}
# predict the classes in the test set 
 
cancer.knn.pred <- predict(cancer.knn.model, cancer.test)
cancer.knn.pred
```
```{r}
# make the confusion table to see how well the model did on the test data compared to the true values 

confusion_cancer <- table(cancer.test$Class, cancer.knn.pred, dnn = c("Real", "Predicted"))
confusion_cancer

```
```{r}

# find the accuracy 

accuracy_cancer <- sum(diag(confusion_cancer)) / sum(confusion_cancer)
accuracy_cancer

```

```{r}
# find the error rate 

error_cancer <- 1 - accuracy_cancer
error_cancer

```
**Present the predictions as a confusion matrix. What’s your conclusion about classifying cancer types from RNA-Seq data so far?**

The confusion matrix and the accuracy shows that the model only gets about 70% right on it's predictions, I would therefore not trust the model at this point. 
As an example: 
The model gets 58 colon samples correctly predicted, but the other colon samples are mistakenly predicted to be from stomach (38) and rectum (3). The model is doing quite good on the predicted stomach samples, which also fits good with the PCA plot where these were the ones best separated. 


```{r, warning=FALSE, message=FALSE, include=FALSE}
# Install the following package
library(randomForest)

iris.rf.model <-  randomForest(Species ~ .,
                               data = iris,
                               importance=TRUE, 
                               ntree=1000)

confusion <- table(iris$Species, iris.rf.model$predicted, dnn = c("Real", "Predicted"))
confusion

```

```{r, include=FALSE}
accuracy <- sum(diag(confusion)) / sum(confusion)
accuracy
```

```{r, include=FALSE}
library(tibble)

iris.rf.model %>% importance() %>% as.data.frame() %>% rownames_to_column(var = "Variables") %>%arrange(desc(MeanDecreaseAccuracy)) %>%  select(Variables, MeanDecreaseAccuracy)

```

<H3> Random Forest on the cancer RNA-seq data <H3/> 


```{r}

cancer.rf.model <- randomForest(classes ~. , data = vst_t, importance = TRUE, ntree = 1000)

confusion_rf <- table(vst_t$Class, cancer.rf.model$predicted, dnn = c("Real","Predicted"))
confusion_rf
```


```{r}
accuracy <- sum(diag(confusion_rf)) / sum(confusion_rf)
accuracy
```

The model with Random Forest show much better accuracy than the model using k-nearest neighbors. 



```{r, warning=FALSE, message=FALSE}

library(dplyr)
library(tidyverse)
library(tibble)

genenames <- read_tsv("genenames.org.txt")

genenames_separated <- genenames %>% select(Symbol,`Ensembl gene ID`) 
colnames(genenames_separated) <- c("Symbol","Genes")

```

```{r}
gene_importance <- cancer.rf.model %>% importance() %>% as.data.frame() %>% rownames_to_column(var = "Genes") %>% mutate(Genes = gsub("\\..*$", "", Genes)) %>% left_join(genenames_separated, by = "Genes") %>% arrange(desc(MeanDecreaseAccuracy)) %>% select(Genes, MeanDecreaseAccuracy, Symbol) %>% filter(Genes != "Class")
gene_importance %>% head(10)
```


**Paste the first 10 gene symbols into your favorite chatbot and ask it to explain why these genes could be relevant - in less than 100 words. Are you convinced?**

The answer from chat-GPT: 
"These genes are relevant because they exhibit high MeanDecreaseAccuracy values in a random forest model, indicating they significantly contribute to the model's ability to classify samples, likely related to specific biological processes or disease states. For instance, HOXB13 is associated with prostate cancer, GATA4 is involved in cardiac development, and MALAT1 is implicated in various cancers. Their identification as important features can guide further research into their roles as potential biomarkers or therapeutic targets in disease contexts."

Chat-GPT gives a quite general answer, but indicates that some of these genes may be relevant in different diseases and cancer types. Further investigation should be done with other sources. 


<H4> Boxplot of 3 of the top 10 most predictive genes <H4/> 


```{r}
library(ggplot2) 

top_genes <- gene_importance %>% arrange(desc(MeanDecreaseAccuracy)) %>% slice(1:10) %>% pull(Genes)

vst_t <- vst %>% t() %>% as.data.frame() %>% mutate(Class = classes) 
colnames(vst_t) <- gsub("\\.[0-9]+$", "", colnames(vst_t))

vst_top_genes <- vst_t %>% select(all_of(top_genes), Class) 

vst_long <- vst_top_genes %>% pivot_longer(cols = -Class, names_to = "Genes", values_to = "Expression")

ggplot(vst_long %>% filter(Genes == "ENSG00000225972"), aes(x = Class, y = Expression, fill = Class)) + geom_boxplot() + labs(title = "Boxplot of ENSG00000225972", x = "Class", y = "Expression") + scale_y_continuous(limits = c(7, 17)) + scale_fill_manual(values = c("Colon" = "pink", "Rectum" = "darkolivegreen2", "Stomach" = "lightblue3")) + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
```


```{r}
ggplot(vst_long %>% filter(Genes == "ENSG00000251562"), aes(x = Class, y = Expression, fill = Class)) + geom_boxplot() + labs(title = "Boxplot of ENSG00000251562", x = "Class", y = "Expression") + scale_y_continuous(limits = c(9, 22)) + scale_fill_manual(values = c("Colon" = "pink", "Rectum" = "darkolivegreen2", "Stomach" = "lightblue3")) + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
```
```{r}
ggplot(vst_long %>% filter(Genes == "ENSG00000159182"), aes(x = Class, y = Expression, fill = Class)) + geom_boxplot() + labs(title = "Boxplot of ENSG00000159182", x = "Class", y = "Expression") + scale_y_continuous(limits = c(6, 15)) + scale_fill_manual(values = c("Colon" = "pink", "Rectum" = "darkolivegreen2", "Stomach" = "lightblue3")) + theme_classic() + theme(plot.title = element_text(hjust = 0.5))
```


<H3> Deep learning <H3/> 

**Explain the architecture of the network: how many nodes are in the input layer, hidden layers and output layers? Why?**

There are 4 nodes in the input layer, 7 nodes (in the code below) in the hidden layers and 
3 nodes in the output layers. 
The input layer are the four input features Sepal length, Sepal width, Petal Length and Petal width. These are put into the model and used to predict the output which in this case is the three different Iris Species. 
In the middle, we have the hidden layers. The hidden layers look at patterns in the data, by applying weights to the input features and sending them to the next layer. Complex models have many hidden layers. Each node or neuron in the hidden layers, are connected to the neurons in the previous and the next layer. 
When the model is trained, the hidden layers adjust their weights to make the predictions more accurate. 

```{r}
# Install package RSNNS, but do not use library(RSNNS) to load it.

iris.mlp.model <- train(Species ~ ., data = iris.train, method = "mlp", trControl = trainControl(method = "cv", number = 5),tuneGrid = data.frame(size = 1:10))

iris.mlp.model

```
```{r, include=FALSE}
plot(iris.mlp.model)
```

```{r, include=FALSE}
iris.mlp.pred <- predict(iris.mlp.model, iris.test)

confusion <- table(iris.test$Species, iris.mlp.pred, dnn = c("Real", "Predicted"))
confusion
```

```{r, include=FALSE}
accuracy <- sum(diag(confusion)) / sum(confusion)
accuracy
```




<H4> Deep learning on the cancer RNA-seq data <H4/> 

```{r}

cancer.mlp.model <- train(Class ~ ., data = cancer.train, method = "mlp", trControl = trainControl(method = "cv", number = 5),tuneGrid = data.frame(size = 10:100))

cancer.mlp.model

```


```{r}

plot(cancer.mlp.model)

```

```{r}
cancer.mlp.pred <- predict(cancer.mlp.model, cancer.test)

confusion_mlp <- table(cancer.test$Class, cancer.mlp.pred, dnn = c("Real", "Predicted"))
confusion_mlp 
```

```{r}
accuracy_mlp <- sum(diag(confusion_mlp)) / sum(confusion_mlp)
accuracy_mlp
```
**What are the number of nodes in the input and the output layer when using the cancer data?**

The number of nodes in the input layer is  275. 
The number of nodes in the output layer is three: Colon, Rectum and Stomach. 

```{r, include=FALSE}
# Install and load the library with svm function
library(e1071)

# Train the model using the training data
iris.svm.model <- svm(Species ~ ., iris.train)

# Use the model to predict the classes for the test data
iris.svm.pred <- predict(iris.svm.model, iris.test)

# Create a confusion matrix
confusion <- table(iris.test$Species, iris.svm.pred, dnn = c("Real", "Predicted"))
confusion
```

```{r, include=FALSE}
# Accuracy
accuracy <- sum(diag(confusion)) / sum(confusion)
accuracy
```





